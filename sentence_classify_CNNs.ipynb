{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentence_classify_CNNs.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"dERxCGF9q0g_","colab_type":"code","outputId":"e6f21395-3c47-4c55-e6f5-30ddb9b72fc2","executionInfo":{"status":"ok","timestamp":1546256058156,"user_tz":-420,"elapsed":87091,"user":{"displayName":"Hưng Nguyễn Tuấn","photoUrl":"","userId":"00297783721254089246"}},"colab":{"base_uri":"https://localhost:8080/","height":216}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 110842 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"4cjje6d3sW6a","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","from six.moves.urllib.request import urlretrieve \n","import pandas as pd\n","import numpy as np\n","import logging\n","import pickle\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7yO0Y00PDX-J","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Lm-cmDOaxoi3","colab_type":"code","colab":{}},"cell_type":"code","source":["!mkdir -p data\n","!google-drive-ocamlfuse data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H2Wri8SrHAfp","colab_type":"code","colab":{}},"cell_type":"code","source":["train_data = pd.read_csv(os.path.join('data','labeledTrainData.tsv'),header=0,\n","                    delimiter=\"\\t\", quoting=3)\n","train_labels = train_data[\"sentiment\"]\n","del train_data\n","train_review_vect = pd.read_pickle(\"data/train_review_vect_100.pickle\")\n","train_review_vect = np.array(train_review_vect)\n","test_review_vect = pd.read_pickle(\"data/test_review_vect_100.pickle\")\n","test_review_vect = np.array(test_review_vect)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"h65Rv_9V1s9T","colab_type":"code","outputId":"2783d5da-2dfa-4594-ddc3-91018f347280","executionInfo":{"status":"ok","timestamp":1546269409990,"user_tz":-420,"elapsed":993,"user":{"displayName":"Hưng Nguyễn Tuấn","photoUrl":"","userId":"00297783721254089246"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"cell_type":"code","source":["num_features = train_review_vect.shape[2]    # Word vector dimensionality       \n","sent_length = train_review_vect.shape[1]\n","num_test_data = test_review_vect.shape[0]\n","num_classes = 2\n","def reformat(labels):\n","  label = (np.arange(num_classes) == labels[:,None]).astype(np.float32)\n","  return label\n","print('Training set', train_review_vect.shape, train_labels.shape)\n","train_labels = reformat(train_labels)\n","print('Testing set', test_review_vect.shape)\n","print('Label', train_labels.shape)\n"],"execution_count":70,"outputs":[{"output_type":"stream","text":["Training set (25000, 100, 80) (25000,)\n","Testing set (25000, 100, 80)\n","Label (25000, 2)\n"],"name":"stdout"}]},{"metadata":{"id":"S3OlQ4BvQxim","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 32\n","graph = tf.Graph()\n","with graph.as_default():\n","  # Different filter sizes we use in a single convolution layer\n","  filter_sizes = [3,5,7] \n","  # inputs and labels\n","  tf_train_sents = tf.placeholder(shape=[batch_size,sent_length,num_features],dtype=tf.float32,name='sentence_inputs')\n","  tf_train_labels = tf.placeholder(shape=[batch_size,num_classes],dtype=tf.float32,name='sentence_labels')\n","  tf_test_sents =  tf.placeholder(shape=[num_test_data,sent_length,num_features],dtype=tf.float32,name='sentence_inputs')\n","\n","  # Weights of the first parallel layer\n","  con_w1 = tf.Variable(tf.truncated_normal([filter_sizes[0],num_features,1],stddev=0.02,dtype=tf.float32),name='weights_1')\n","  con_b1 = tf.Variable(tf.random_uniform([1],0,0.01,dtype=tf.float32),name='bias_1')\n","\n","  # Weights of the second parallel layer\n","  con_w2 = tf.Variable(tf.truncated_normal([filter_sizes[1],num_features,1],stddev=0.02,dtype=tf.float32),name='weights_2')\n","  con_b2 = tf.Variable(tf.random_uniform([1],0,0.01,dtype=tf.float32),name='bias_2')\n","\n","  # Weights of the third parallel layer\n","  con_w3 = tf.Variable(tf.truncated_normal([filter_sizes[2],num_features,1],stddev=0.02,dtype=tf.float32),name='weights_3')\n","  con_b3 = tf.Variable(tf.random_uniform([1],0,0.01,dtype=tf.float32),name='bias_3')\n","\n","  # Fully connected layer\n","  fc_w1 = tf.Variable(tf.truncated_normal([len(filter_sizes),num_classes],stddev=0.5,dtype=tf.float32),name='weights_fulcon_1')\n","  fc_b1 = tf.Variable(tf.random_uniform([num_classes],0,0.01,dtype=tf.float32),name='bias_fulcon_1')\n","\n","  def model(data):\n","\n","    # Calculate the output for all the filters with a stride 1\n","    # We use relu activation as the activation function\n","    conv1 =tf.nn.conv1d(data,con_w1,stride=1,padding='SAME')\n","    hidden1_1 = tf.nn.relu(conv1 + con_b1)\n","    conv2 =tf.nn.conv1d(data,con_w2,stride=1,padding='SAME')\n","    hidden1_2 = tf.nn.relu(conv2 + con_b2)\n","    conv3 =tf.nn.conv1d(data,con_w3,stride=1,padding='SAME')\n","    hidden1_3 = tf.nn.relu(conv3 + con_b3)\n","   \n","    # Pooling over time operation\n","\n","    # This is doing the max pooling. Thereare two options to do the max pooling\n","    # 1. Use tf.nn.max_pool operation on a tensor made by concatenating h1_1,h1_2,h1_3 and converting that tensor to 4D\n","    # (Because max_pool takes a tensor of rank >= 4 )\n","    # 2. Do the max pooling separately for each filter output and combine them using tf.concat \n","    # (this is the one used in the code)\n","\n","    hidden2_1 = tf.reduce_mean(hidden1_1,axis=1)\n","    hidden2_2 = tf.reduce_mean(hidden1_2,axis=1)\n","    hidden2_3 = tf.reduce_mean(hidden1_3,axis=1)\n","\n","    hidden2 = tf.concat([hidden2_1,hidden2_2,hidden2_3],axis=1)\n","    return tf.matmul(hidden2,fc_w1) + fc_b1\n","\n","  # Calculate the fully connected layer output (no activation)\n","  # Note: since h2 is 2d [batch_size,number of parallel filters] \n","  # reshaping the output is not required as it usually do in CNNs\n","  logits = model(tf_train_sents)\n","  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels,logits=logits))\n","\n","  # Momentum Optimizer\n","  optimizer = tf.train.MomentumOptimizer(learning_rate=0.01,momentum=0.9).minimize(loss)\n","#   predictions = tf.nn.softmax(model(tf_test_sents))\n","  predictions = tf.argmax(tf.nn.softmax(model(tf_test_sents)),axis=1)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bX5Yoj0MR58d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1008},"outputId":"7317b118-9f75-4dba-92bd-3f2175d0d073","executionInfo":{"status":"ok","timestamp":1546270575763,"user_tz":-420,"elapsed":1156500,"user":{"displayName":"Hưng Nguyễn Tuấn","photoUrl":"","userId":"00297783721254089246"}}},"cell_type":"code","source":["num_steps = 4001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.initialize_all_variables().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_review_vect[offset:(offset + batch_size), :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_sents : batch_data, tf_train_labels : batch_labels, tf_test_sents : test_review_vect}\n","    _, l = session.run(\n","      [optimizer, loss], feed_dict=feed_dict)\n","    if (step % 50 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","    if(l < 0.2):\n","      print(l)\n","      break\n","  _, l,predictions = session.run(\n","      [optimizer, loss,predictions], feed_dict=feed_dict)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 0.692594\n","Minibatch loss at step 50: 0.694756\n","Minibatch loss at step 100: 0.690060\n","Minibatch loss at step 150: 0.687073\n","Minibatch loss at step 200: 0.682010\n","Minibatch loss at step 250: 0.686916\n","Minibatch loss at step 300: 0.675879\n","Minibatch loss at step 350: 0.665570\n","Minibatch loss at step 400: 0.671814\n","Minibatch loss at step 450: 0.651121\n","Minibatch loss at step 500: 0.651968\n","Minibatch loss at step 550: 0.603152\n","Minibatch loss at step 600: 0.613374\n","Minibatch loss at step 650: 0.564644\n","Minibatch loss at step 700: 0.558190\n","Minibatch loss at step 750: 0.492209\n","Minibatch loss at step 800: 0.535086\n","Minibatch loss at step 850: 0.580713\n","Minibatch loss at step 900: 0.564148\n","Minibatch loss at step 950: 0.518514\n","Minibatch loss at step 1000: 0.532076\n","Minibatch loss at step 1050: 0.471510\n","Minibatch loss at step 1100: 0.485402\n","Minibatch loss at step 1150: 0.542980\n","Minibatch loss at step 1200: 0.508503\n","Minibatch loss at step 1250: 0.522172\n","Minibatch loss at step 1300: 0.532190\n","Minibatch loss at step 1350: 0.356695\n","Minibatch loss at step 1400: 0.462432\n","Minibatch loss at step 1450: 0.492926\n","Minibatch loss at step 1500: 0.445547\n","Minibatch loss at step 1550: 0.392263\n","Minibatch loss at step 1600: 0.614024\n","Minibatch loss at step 1650: 0.392243\n","Minibatch loss at step 1700: 0.366401\n","Minibatch loss at step 1750: 0.399330\n","Minibatch loss at step 1800: 0.483726\n","Minibatch loss at step 1850: 0.305476\n","Minibatch loss at step 1900: 0.386583\n","Minibatch loss at step 1950: 0.264827\n","Minibatch loss at step 2000: 0.507156\n","Minibatch loss at step 2050: 0.365180\n","Minibatch loss at step 2100: 0.421027\n","Minibatch loss at step 2150: 0.407643\n","Minibatch loss at step 2200: 0.511985\n","Minibatch loss at step 2250: 0.379198\n","Minibatch loss at step 2300: 0.457271\n","Minibatch loss at step 2350: 0.366939\n","Minibatch loss at step 2400: 0.348898\n","Minibatch loss at step 2450: 0.360115\n","Minibatch loss at step 2500: 0.564829\n","Minibatch loss at step 2550: 0.538829\n","Minibatch loss at step 2600: 0.537657\n","Minibatch loss at step 2650: 0.320454\n","Minibatch loss at step 2700: 0.453020\n","Minibatch loss at step 2750: 0.329431\n","Minibatch loss at step 2800: 0.514362\n","Minibatch loss at step 2850: 0.402346\n","Minibatch loss at step 2900: 0.471516\n","0.1858374\n"],"name":"stdout"}]},{"metadata":{"id":"EiA8K7UKf3IR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"7cb60720-cd0d-4c86-89b8-fec0b874a278","executionInfo":{"status":"ok","timestamp":1546271107061,"user_tz":-420,"elapsed":955,"user":{"displayName":"Hưng Nguyễn Tuấn","photoUrl":"","userId":"00297783721254089246"}}},"cell_type":"code","source":["np.sum(predictions)\n","\n"],"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13195"]},"metadata":{"tags":[]},"execution_count":74}]},{"metadata":{"id":"klcAk3TXydKt","colab_type":"code","colab":{}},"cell_type":"code","source":["# del test_review_vect\n","# del train_review_vect\n","test_data = pd.read_csv(os.path.join(os.path.dirname('__file__'),'data','testData.tsv' ),header=0,\n","                    delimiter=\"\\t\", quoting=3)\n","\n","output = pd.DataFrame(data={\"id\":test_data[\"id\"], \"sentiment\":predictions})\n","output.to_csv( \"data/CNN_mean.csv\", index=False, quoting=3 )"],"execution_count":0,"outputs":[]}]}